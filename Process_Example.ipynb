{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a267f912",
   "metadata": {},
   "source": [
    "Paper PDF -> JSON Pipeline & COT Generation\n",
    "\n",
    "This notebook contains two main parts:\n",
    "1. Extract text from PDF and split it into JSON paragraphs;\n",
    "2. Call a large model (OpenAI-like interface) to generate a chain-of-thought (CoT) for each paragraph and save the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65a62db",
   "metadata": {},
   "source": [
    "Instructions and Precautions:\n",
    "- Please install dependencies according to your actual environment (pdfminer.six, natsort, openai, or the SDK for the large model you are using).\n",
    "- To avoid rate limits or unexpected charges, it is recommended to test on a small number of samples first (for example, set start_index to a small number).\n",
    "- If PDF extraction quality is not high, it is recommended to use OCR tools (such as Tesseract) or manually proofread key sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733b9763",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c6fae0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pdfminer.six natsort openai\n",
    "\n",
    "!pip install gradio transformers accelerate\n",
    "\n",
    "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "%cd LLaMA-Factory\n",
    "!pip install -e \"LLaMA-Factory.[torch,metrics]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66413f3f",
   "metadata": {},
   "source": [
    "Configuration\n",
    "\n",
    "Enter your API Key, base_url, model name, and data path in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe71d58",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "API_KEY = \"\" # Enter your API key\n",
    "BASE_URL = \"\" # Enter the address of your calling model\n",
    "MODEL_NAME = \"\" # Enter the model you wish to call\n",
    "\n",
    "# File path (adjust according to your directory)\n",
    "INPUT_PDF_FOLDER = \"user/paper_dataset_perovskite/pdf\"\n",
    "OUTPUT_JSON_FOLDER = \"user/paper_dataset_perovskite/json_split\"\n",
    "MERGED_JSON_FILE = \"user/paper_dataset_perovskite/paper_split.json\"\n",
    "FAIL_PATH = \"user/paper_dataset_perovskite/fail.txt\"\n",
    "\n",
    "# CoT Output Path\n",
    "COT_OUTPUT_FILE = \"user/dataset_perovskite/new/paper_split_cot_new.json\"\n",
    "\n",
    "# Other Settings\n",
    "MAX_LENGTH = 2500\n",
    "SLEEP_TIME = 1\n",
    "START_INDEX = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0318bcf2",
   "metadata": {},
   "source": [
    "Part One: Extracting Text from PDF and Splitting into JSON Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a978d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from glob import glob\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "from natsort import natsorted\n",
    "\n",
    "def extract_pdf_content(pdf_path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    outfp = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr=rsrcmgr, outfp=outfp, laparams=laparams)\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as fp:\n",
    "            interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "            password = \"\"\n",
    "            maxpages = 0\n",
    "            caching = True\n",
    "            pagenos = set()\n",
    "            for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password, caching=caching, check_extractable=True):\n",
    "                try:\n",
    "                    interpreter.process_page(page)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing page in {pdf_path}: {e}\", flush=True)\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening or processing {pdf_path}: {e}\", flush=True)\n",
    "        return \"\"\n",
    "    finally:\n",
    "        try:\n",
    "            mystr = outfp.getvalue()\n",
    "        except Exception:\n",
    "            mystr = \"\"\n",
    "        try:\n",
    "            device.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            outfp.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return mystr\n",
    "\n",
    "def split_content_by_words(content, max_length=MAX_LENGTH):\n",
    "    words = content.split()\n",
    "    content_parts = []\n",
    "    step = int(max_length * 0.8) \n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        part = \" \".join(words[i:i + max_length])\n",
    "        content_parts.append(part)\n",
    "        i += step\n",
    "    return content_parts\n",
    "\n",
    "def process_pdfs(input_folder, output_folder, fail_path):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    s = 0\n",
    "    w = 0\n",
    "    failed_files = []\n",
    "\n",
    "    for filename in natsorted(os.listdir(input_folder)):\n",
    "        if not filename.lower().endswith('.pdf'):\n",
    "            continue\n",
    "\n",
    "        pdf_path = os.path.join(input_folder, filename)\n",
    "        print(f\"Processing {pdf_path}...\")\n",
    "\n",
    "\n",
    "        content = extract_pdf_content(pdf_path)\n",
    "        if not content:\n",
    "            print(f\"No content extracted from {pdf_path}, skipping.\")\n",
    "            w += 1\n",
    "            failed_files.append(pdf_path)\n",
    "            continue\n",
    "\n",
    "\n",
    "        content_parts = split_content_by_words(content)\n",
    "        base_filename = os.path.splitext(filename)[0]\n",
    "\n",
    "\n",
    "        for i, part in enumerate(content_parts):\n",
    "            json_filename = f\"{base_filename}_part{i+1}.json\"\n",
    "            json_path = os.path.join(output_folder, json_filename)\n",
    "            os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "\n",
    "\n",
    "            try:\n",
    "                with open(json_path, 'w', encoding='utf-8') as json_file:\n",
    "                    json.dump({\"content\": part}, json_file, ensure_ascii=False, indent=4)\n",
    "                print(f\"Saved JSON part {i+1} to {json_path}\", flush=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving JSON file {json_path}: {e}\", flush=True)\n",
    "                w += 1\n",
    "\n",
    "        s += 1\n",
    "\n",
    "    try:\n",
    "        with open(os.path.join(output_folder, os.path.basename(fail_path)), \"w\", encoding='utf-8') as f:\n",
    "            if failed_files:\n",
    "                f.write(\"Failed files:\\n\")\n",
    "                for file in failed_files:\n",
    "                    f.write(f\"- {file}\\n\")\n",
    "                f.write(f\"\\nTotal failed files: {len(failed_files)}\")\n",
    "            else:\n",
    "                f.write(\"No files failed.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing fail file: {e}\", flush=True)\n",
    "\n",
    "    print(f\"{s} files successfully processed.\")\n",
    "    print(f\"{w} files skipped.\")\n",
    "\n",
    "def merge_json_files(json_folder, output_file):\n",
    "    merged_data = []\n",
    "    json_files = natsorted(glob(os.path.join(json_folder, \"*.json\")))\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, \"r\", encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    merged_data.extend(data)\n",
    "                else:\n",
    "                    merged_data.append(data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON file {json_file}: {e}\", flush=True)\n",
    "\n",
    "    try:\n",
    "        with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "            json.dump(merged_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"All JSON files merged into {output_file}\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving merged JSON file {output_file}: {e}\", flush=True)\n",
    "\n",
    "def delete_folder(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        try:\n",
    "            shutil.rmtree(folder_path)\n",
    "            print(f\"Folder {folder_path} deleted successfully.\", flush=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting folder {folder_path}: {e}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3c1d64",
   "metadata": {},
   "source": [
    "Run Example: Process PDF and Merge into a Single JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02732072",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "process_pdfs(INPUT_PDF_FOLDER, OUTPUT_JSON_FOLDER, FAIL_PATH)\n",
    "merge_json_files(OUTPUT_JSON_FOLDER, MERGED_JSON_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79267107",
   "metadata": {},
   "source": [
    "Part Two: Using Large Language Models to Generate Chain-of-Thought (CoT) for Each Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8be56a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception:\n",
    "    OpenAI = None\n",
    "\n",
    "client = None\n",
    "if OpenAI is not None and API_KEY:\n",
    "    client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "else:\n",
    "    print(\"Warning: OpenAI client not initialized. Please check the API_KEY / environment and dependencies.\")\n",
    "\n",
    "def generate_cot_from_merged(merged_json_path, output_path, start_index=START_INDEX, model_name=MODEL_NAME, client_obj=None, sleep_time=SLEEP_TIME):\n",
    "    if client_obj is None:\n",
    "        raise RuntimeError(\"Model client is not initialized. Please set API_KEY and ensure SDK is installed.\")\n",
    "\n",
    "    with open(merged_json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        cot_set = []\n",
    "    for idx, item in enumerate(data):\n",
    "        if idx < start_index:\n",
    "            continue\n",
    "\n",
    "        excerpt = item.get('content', '')\n",
    "        print(\"The Number:\", idx, flush=True)\n",
    "\n",
    "        prompt_user = (\n",
    "            \"You are an expert in the field of perovskite. Your task is to generate chain of thought for the construction of dataset.\\n\"\n",
    "            \"Provide step-by-step reasoning, key considerations, typical experimental parameters (where relevant), and suggestions for dataset construction.\"\n",
    "        )\n",
    "        prompt_system = \"Read and think carefully about the fragment. Generate the suitable output.\"\n",
    "\n",
    "        try:\n",
    "            completion = client_obj.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": prompt_system},\n",
    "                    {\"role\": \"user\", \"content\": prompt_user + \"\\n\\nFragment:\\n\" + excerpt},\n",
    "                ],\n",
    "                temperature=8.0,\n",
    "            )\n",
    "            output = completion.choices[0].message.content\n",
    "            print(output, \"\\n\", flush=True)\n",
    "        except Exception as e:\n",
    "            output = f\"Error: {str(e)}\"\n",
    "            print(f\"Unexpected error at index {idx}: {str(e)}\", flush=True)\n",
    "\n",
    "        cot_set.append({\"id\": idx, \"cot\": output})\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(cot_set, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"\\ndone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7912a97e",
   "metadata": {},
   "source": [
    "Run example: Generate CoT for the merged JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276ff4c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "generate_cot_from_merged(MERGED_JSON_FILE, COT_OUTPUT_FILE, start_index=START_INDEX, model_name=MODEL_NAME, client_obj=client, sleep_time=SLEEP_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b94eaec",
   "metadata": {},
   "source": [
    "Part Three: Training model\n",
    "\n",
    "Set reasonable parameters in the YAML file and start training the model. You can refer to this website to set up the appropriate YAML file: https://llamafactory.readthedocs.io/en/latest/getting_started/sft.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a041d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f7932a",
   "metadata": {},
   "source": [
    "Part Four: Interact with the Perovskite-R1 model. Load the model in the Notebook, and launch the Gradio chat interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d66314",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import re\n",
    "\n",
    "MODEL_PATH = \"JH976/Perovskite-R1\" # or a locally trained model\n",
    "\n",
    "print(\"Loading model...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Model loading failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# %% \n",
    "def predict(message, history):\n",
    "    messages = []\n",
    "    for user_turn, bot_turn in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_turn})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": bot_turn})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=8192,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.8\n",
    "    )\n",
    "\n",
    "    response_ids = outputs[0][input_ids.shape[-1]:]\n",
    "    response_text = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "\n",
    "    clean_response = re.sub(r'<think>.*?</think>\\s*\\n*', '', response_text, flags=re.DOTALL)\n",
    "    return clean_response.strip()\n",
    "\n",
    "# %%\n",
    "print(\"Starting Gradio...\")\n",
    "demo = gr.ChatInterface(\n",
    "    fn=predict,\n",
    "    title=\"Perovskite-R1\",\n",
    "    description=\"Enter your question to have a conversation with the LLM.\",\n",
    "    theme=\"soft\",\n",
    "    examples=[[\"Hello\"], [\"Do you know about perovskite?\"]],\n",
    ")\n",
    "\n",
    "# %%\n",
    "# launch Gradioï¼Œshare=True can generate a public access link\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
