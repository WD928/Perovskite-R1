# Perovskite-R1: A Domain-Specialized LLM for Intelligent Discovery of Precursor Additives

[![arXiv](https://img.shields.io/badge/arXiv-Paper-b31b1b.svg)](https://arxiv.org/abs/24xx.xxxxx) 
[![Hugging Face Datasets](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face-Datasets-yellow.svg)](https://huggingface.co/datasets/JH976/Perovskite-R1)
[![Hugging Face Model](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face-Model-blue.svg)](https://huggingface.co/JH976/Perovskite-R1)
[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](./LICENSE)

This is the official repository for the paper **"Perovskite-R1: A Domain-Specialized LLM for Intelligent Discovery of Precursor Additives and Experimental Design"**.

Perovskite-R1 is a large language model fine-tuned on **QwQ-32B**, specifically designed to assist materials scientists in perovskite synthesis planning, precursor selection, and experimental optimization.

---

## ðŸ”— Quick Links

| Resource | Description | Link |
| :--- | :--- | :--- |
| **Paper** | The full manuscript on arXiv | [Read Paper](https://arxiv.org/abs/2507.16307) |
| **Model** | Perovskite-R1 model weights | [Hugging Face](https://huggingface.co/JH976/Perovskite-R1) |
| **Datasets** | Training set & Task-specific Benchmark | [Hugging Face](https://huggingface.co/datasets/JH976/Perovskite-R1) |
| **Validation** | **Raw outputs, expert reviews & logs** | [Go to Folder](./Validation_Records) |

---

## ðŸ“‚ Repository Structure

```text
Perovskite-R1/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py                  # Gradio-based web interface (supports Thinking Process visualization)
â”‚   â””â”€â”€ requirements.txt        # Dependencies for the demo application
â”œâ”€â”€ Preprocess/
â”‚   â”œâ”€â”€ gen_paper_cot.py        # Generates Chain-of-Thought (CoT) data using OpenAI o1
â”‚   â””â”€â”€ pdf2json.py             # Parses PDF literature into structured JSON format
â”œâ”€â”€ Validation_Records/
â”‚   â”œâ”€â”€ check_contamination.py  # Script for data integrity and contamination analysis
â”‚   â”œâ”€â”€ Raw_Model_Outputs.../   # Inference logs for benchmarks
â”‚   â””â”€â”€ Human_Expert...         # Expert evaluation records
â”œâ”€â”€ Process_Example.ipynb       # End-to-end tutorial (Preprocessing -> Training -> Inference)
â””â”€â”€ README.md            
```

## ðŸš€ Interactive Demo

We provide a **Gradio-based web interface** that visualizes the model's reasoning process (Chain-of-Thought) separate from the final answer.

### Setup & Run
1. Install dependencies:
   ```bash
   pip install -r app/requirements.txt
   ```
2. Run the application (specify your model path):
   ```bash
   python app/app.py --model_path /path/to/your/model
   ```

## ðŸ›  Data Processing Pipeline

The `Preprocess/` folder contains the core scripts used to construct the domain-specific dataset:

* **`pdf2json.py`**: Extracts raw text and metadata from scientific PDFs, converting them into a structured JSON format suitable for training.
* **`gen_paper_cot.py`**: A distillation script that utilizes the **OpenAI o1** model to generate high-quality Chain-of-Thought (CoT) reasoning paths based on the raw text.
