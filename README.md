# Perovskite-R1: A Domain-Specialized LLM for Intelligent Discovery of Precursor Additives

[![arXiv](https://img.shields.io/badge/arXiv-Paper-b31b1b.svg)](https://arxiv.org/abs/24xx.xxxxx) 
[![Hugging Face Datasets](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face-Datasets-yellow.svg)](https://huggingface.co/datasets/JH976/Perovskite-R1)
[![Hugging Face Model](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face-Model-blue.svg)](https://huggingface.co/JH976/Perovskite-R1)
[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](./LICENSE)

This is the official repository for the paper **"Perovskite-R1: A Domain-Specialized LLM for Intelligent Discovery of Precursor Additives and Experimental Design"**.

Perovskite-R1 is a large language model fine-tuned on **QwQ-32B**, specifically designed to assist materials scientists in perovskite synthesis planning, precursor selection, and experimental optimization.

---

## ðŸ”— Quick Links

| Resource | Description | Link |
| :--- | :--- | :--- |
| **Paper** | The full manuscript on arXiv | [Read Paper](https://arxiv.org/abs/2507.16307) |
| **Model** | Perovskite-R1 model weights | [Hugging Face](https://huggingface.co/JH976/Perovskite-R1) |
| **Datasets** | Training set & Task-specific Benchmark | [Hugging Face](https://huggingface.co/datasets/JH976/Perovskite-R1) |
| **Validation** | **Raw outputs, expert reviews & logs** | [Go to Folder](./Validation_Records) |

---

## ðŸ“‚ Repository Structure

```text
Perovskite-R1/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py                  # Gradio-based web interface (supports Thinking Process visualization)
â”‚   â””â”€â”€ requirements.txt        # Dependencies for the demo application
â”œâ”€â”€ Preprocess/
â”‚   â”œâ”€â”€ gen_paper_cot.py        # Generates Chain-of-Thought (CoT) data using OpenAI o1
â”‚   â””â”€â”€ pdf2json.py             # Parses PDF literature into structured JSON format
â”œâ”€â”€ Validation_Records/
â”‚   â”œâ”€â”€ check_contamination.py  # Script for data integrity and contamination analysis
â”‚   â”œâ”€â”€ Raw_Model_Outputs.../   # Inference logs for benchmarks
â”‚   â””â”€â”€ Human_Expert...         # Expert evaluation records
â”œâ”€â”€ Process_Example.ipynb       # End-to-end tutorial (Preprocessing -> Training -> Inference)
â””â”€â”€ README.md            
```

## ðŸš€ Interactive Demo

We provide a **Gradio-based web interface** that visualizes the model's reasoning process (Chain-of-Thought) separate from the final answer.

### Setup & Run
1. Install dependencies:
   ```bash
   pip install -r app/requirements.txt
   ```
2. Run the application (specify your model path):
   ```bash
   python app/app.py --model_path /path/to/your/model
   ```

## ðŸ›  Data Processing Pipeline

The `Preprocess/` folder contains the core scripts used to construct the domain-specific dataset:

* **`pdf2json.py`**: Extracts raw text and metadata from scientific PDFs, converting them into a structured JSON format suitable for training.
* **`gen_paper_cot.py`**: A distillation script that utilizes the **OpenAI o1** model to generate high-quality Chain-of-Thought (CoT) reasoning paths based on the raw text.


## ðŸ“– End-to-End Tutorial

For a complete walkthrough of our methodology, please refer to **[Process_Example.ipynb](./Process_Example.ipynb)**.

This Jupyter Notebook demonstrates the entire workflow, including:
1.  Data Preprocessing and formatting.
2.  Fine-tuning configuration using LLaMA-Factory.
3.  Inference examples.

## ðŸ“Š Validation & Integrity

We prioritize transparency and scientific rigor. The **[Validation_Records](./Validation_Records)** directory contains:

* **`check_contamination.py`**: A script to rigorously verify that there is no data leakage between the training set and the benchmark (using N-gram and Semantic analysis).
* **Raw Logs**: Complete inference outputs for Perovskite-R1 and baseline models.
* **Expert Evaluations**: Records of the blind side-by-side human expert review.

For more details on the validation metrics, please check the [README inside the folder](./Validation_Records/README.md).

## ðŸ“œ Citation

If you use this code or model in your research, please cite our paper:

```bibtex
@article{wang2025perovskite,
  title={Perovskite-R1: A Domain-Specialized LLM for Intelligent Discovery of Precursor Additives and Experimental Design},
  author={Wang, Xin-De and Chen, Zhi-Rui and Guo, Peng-Jie and Gao, Ze-Feng and Mu, Cheng and Lu, Zhong-Yi},
  journal={arXiv preprint arXiv:2507.16307},
  year={2025}
}
